---
title: "Sick dataset analysis"
author: "Hubert Baniecki"
date: "`r Sys.Date()`"
output:
  rmdformats::material:
    highlight: kate
    self_contained: true
    code_folding: show
    thumbnails: false
    gallery: true
    fig_width: 10
    fig_height: 6
    df_print: kable
---
  
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align='center', message = FALSE, warning = FALSE)

library(OpenML)
library(tidyverse)
library(kableExtra)
library(visdat)
library(naniar)
library(DataExplorer)
library(patchwork)
library(PRROC)  #https://stats.stackexchange.com/a/226972
library(DALEX)
library(rpart)
```


```{r data, include = FALSE}
openml_id <- 38 
dataset_openml <- getOMLDataSet(data.id = openml_id)
dataset_raw <- dataset_openml$data
target_column <- dataset_openml$target.features

index_raw <- read.table('index.txt')
index <- unname(unlist(index_raw))
```

# Preprocessing

## Remove columns:

- `TBG`  has one value
- `hypopituitary`  has only one value (apart form 1 row)
- `TBG_measured`  is all  `NA`

```{r preprocessing1}
dataset_clean <- dataset_raw %>%
  select(-TBG, -TBG_measured, -hypopituitary) # one value / NA only
```

## Transform columns:

- `M, t, sick`  => `1`
- `F, f, negative`  => `0`

```{r preprocessing2}
temp <- dataset_clean

for (i in 1:ncol(temp)) {
  if (!is.null(levels(temp[[i]]))) {
    if (all(levels(temp[[i]]) %in% c("f", "t"))) {
      temp[i] <- as.numeric(ifelse(temp[i] == 't', 1, 0))
    }   
  }
}
temp$sex <- ifelse(temp$sex == "M", 1, 0)
temp$Class <- as.factor(ifelse(temp$Class == "sick", 1, 0))

dataset <- temp
```

## Basic imputation (common sense):

- `age` :  `454` and `NA` => `mean(age)`  (2 rows)
- `sex` :  if  `pregnant==1`  then  `NA` => `0`   (2 rows)  

```{r preprocessing3}
dataset$age[dataset$age>100] <- mean(dataset$age[dataset$age<100], na.rm = TRUE) 
dataset$age[is.na(dataset$age)] <- mean(dataset$age, na.rm = TRUE)
dataset$sex[dataset$pregnant==1] <- 0
```

```{r missings, eval=FALSE, include = FALSE}
# missing <- DataExplorer::plot_missing(dataset)
# missing_info <- as.data.frame(missing$data)
# missing_columns <- as.character(missing_info$feature[missing_info$num_missing>0])
# full_columns <- setdiff(colnames(dataset_clean), missing_columns)
# 
# print(paste0("missing columns: ", stringi::stri_paste(missing_columns, collapse=", ")))
# print(paste0("full columns: ", stringi::stri_paste(full_columns, collapse=", ")))
```

# Base Models 

## base lm model

It was bad. 

```{r baselm, eval=FALSE, include = FALSE}
## base lm model
# temp <- dataset
# temp$Class <- as.numeric(as.character(temp$Class))
# temp <- DataExplorer::set_missing(temp, -1)
# m_base <- glm(Class~., data = temp[index, ], family = 'binomial')
# 
# library(DALEX)
# exp_base <- explain(m_base, data = temp[-index, ], y = temp$Class[-index])
# model_performance(exp_base)
# 
# prob <- exp_base$y_hat
# y_truth <- exp_base$y
# positive_value <- 1
# auprc::auprc(prob, y_truth, positive_value)
# auprc::precision_recall_curve(prob, y_truth, positive_value)
```

## base rpart model

```{r baserpart}
temp <- dataset
temp$Class <- as.numeric(as.character(temp$Class))
train <- temp[index, ]
test <- temp[-index, ]

base_rpart <- rpart(Class~., data = train, method="anova", model = TRUE)
```

### The tree

```{r}
rpart.plot::rpart.plot(base_rpart)
```

<!-- ### Dominik auprc -->

<!-- ```{r} -->
<!-- prob <- exp_base_rpart$y_hat -->
<!-- y_truth <- exp_base_rpart$y -->
<!-- positive_value <- 1 -->
<!-- auprc::auprc(prob, y_truth, positive_value) -->
<!-- auprc::precision_recall_curve(prob, y_truth, positive_value) -->
<!-- ``` -->

### test

```{r}
exp_base_rpart <- explain(base_rpart,
                          data = test,
                          y = test$Class,
                          model_info = list(type="classification"),
                          verbose=FALSE)

mp_test <- model_performance(exp_base_rpart)
mp_test$measures[c('auc')]
```

Calculate auprc.

```{r}
prob <- predict(base_rpart, test)
y_truth <- test$Class

fg <- prob[y_truth == 1]
bg <- prob[y_truth == 0]

# PR Curve
pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
pr$auc.integral
plot(pr)
```

### train

```{r}
exp_train<- explain(base_rpart,
                          data = train,
                          y = train$Class,
                          model_info = list(type="classification"),
                          verbose=FALSE)

mp_train <- model_performance(exp_train)
mp_train$measures[c('auc')]
```

Calculate auprc.

```{r}
prob <- exp_train$y_hat
y_truth <- exp_train$y

fg <- prob[y_truth == 1]
bg <- prob[y_truth == 0]

# PR Curve
pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
pr$auc.integral
plot(pr)
```

## Feature importance

Which columns are important? Can we remove some?

```{r}
data.frame(variable_importance=round(base_rpart$variable.importance,3))
```

# EDA

## vis_binary

Check if there are very similar columns (not really).

```{r}
dat_bin <- train %>% select(sex, on_thyroxine, query_on_thyroxine, on_antithyroid_medication, sick,
                              pregnant, thyroid_surgery, I131_treatment, query_hypothyroid, query_hyperthyroid,
                              lithium, goitre, tumor, psych, TSH_measured, T3_measured, TT4_measured, T4U_measured,
                              FTI_measured, Class)
visdat::vis_binary(dat_bin)

```

## Variable vs Target plots

I aim to delete all binary columns. For every column I count `Class=1` rows in the minority class of this column.

### referral_source

```{r echo=FALSE}
ggstatsplot::ggpiestats(
  data = train,
  x = Class,
  y = referral_source,
  messages = FALSE)
```

```{r}
sum(train$referral_source[train$Class==1]=="STMW")
```

### tumor

```{r echo=FALSE}
ggstatsplot::ggpiestats(
  data = train,
  x = Class,
  y = tumor,
  messages = FALSE)
```

```{r}
sum(train$tumor[train$Class==1]==1)
```

### I131_treatment

```{r echo=FALSE}
ggstatsplot::ggpiestats(
  data = train,
  x = Class,
  y = I131_treatment,
  messages = FALSE)
```

```{r}
sum(train$I131_treatment[train$Class==1]==1)
```

### thyroid_surgery

```{r echo=FALSE}
ggstatsplot::ggpiestats(
  data = train,
  x = Class,
  y = thyroid_surgery,
  messages = FALSE)
```

```{r}
sum(train$thyroid_surgery[train$Class==1]==1)
```

### on_antithyroid_medication

```{r echo=FALSE}
ggstatsplot::ggpiestats(
  data = train,
  x = Class,
  y = on_antithyroid_medication,
  messages = FALSE)
```

```{r}
sum(train$on_antithyroid_medication[train$Class==1]==1)
```

### pregnant

```{r echo=FALSE}
ggstatsplot::ggpiestats(
  data = train,
  x = Class,
  y = pregnant,
  messages = FALSE)
```

```{r}
sum(train$pregnant[train$Class==1]==1)
```

### goitre

```{r echo=FALSE}
ggstatsplot::ggpiestats(
  data = train,
  x = Class,
  y = goitre,
  messages = FALSE)
```

```{r}
sum(train$goitre[train$Class==1]==1)
```

### lithium

```{r echo=FALSE}
ggstatsplot::ggpiestats(
  data = train,
  x = Class,
  y = lithium,
  messages = FALSE)
```

```{r}
sum(train$lithium[train$Class==1]==1)
```

Not present columns had a lot more diversity. **Leave unchanged: sex, sick, psych, on_thyroxine, query_on_thyroxine, query_hypothyroid, query_hyperthyroid.** 


## Measured vs Values

Here I focus on `TSH, TT4, T3, T4U, FTI` and corresponding `_measured` columns. Not measured means no value in the corresponding column. (measured `FALSE` => value `NA`)

At first, I checked the density of `value` for each `Class`. I later tried to impute `mean(column)` for specific `Class=1/0` but it didn't change anything. 

Secondly, I checked `_measured` columns . Below I aim to do the same as in the previous section.

### TSH

```{r echo=FALSE}
ggstatsplot::ggbetweenstats(
    data = dataset,
    x = Class,
    y = TSH,
  messages = FALSE)

ggstatsplot::ggpiestats(
  data = train,
  x = Class,
  y = TSH_measured,
  messages = FALSE)
```

```{r}
sum(train$TSH_measured[train$Class==1]==0)
```

### TT4

```{r echo=FALSE}
ggstatsplot::ggbetweenstats(
    data = dataset,
    x = Class,
    y = TT4,
  messages = FALSE)

ggstatsplot::ggpiestats(
  data = train,
  x = Class,
  y = TT4_measured,
  messages = FALSE)
```

```{r}
sum(train$TT4_measured[train$Class==1]==0)
```

### T3

```{r echo=FALSE}
ggstatsplot::ggbetweenstats(
    data = dataset,
    x = Class,
    y = T3,
  messages = FALSE)

ggstatsplot::ggpiestats(
  data = train,
  x = Class,
  y = T3_measured,
  messages = FALSE)
```

```{r}
sum(train$T3_measured[train$Class==1]==0)
```

### T4U

```{r echo=FALSE}
ggstatsplot::ggbetweenstats(
    data = dataset,
    x = Class,
    y = T4U,
  messages = FALSE)

ggstatsplot::ggpiestats(
  data = train,
  x = Class,
  y = T4U_measured,
  messages = FALSE)
```

```{r}
sum(train$T4U_measured[train$Class==1]==0)
```

### FTI

```{r echo=FALSE}
ggstatsplot::ggbetweenstats(
    data = dataset,
    x = Class,
    y = FTI,
  messages = FALSE)

ggstatsplot::ggpiestats(
  data = train,
  x = Class,
  y = FTI_measured,
  messages = FALSE)
```

```{r}
sum(train$FTI_measured[train$Class==1]==0)
```


# Experiments

## Experiment 1 - use EDA

Now, I use the information from `EDA` to filter `train` rows before training the model and `test` rows before making the prediction. 

After removing these rows, 13 columns will have only one value, so they can be removed.

### pp()

Filter "for sure* negative" and  remove 13 one-value columns.

```{r}
pp <- function(X) {
  X1 <- X %>% filter(
    referral_source=="STMW" | tumor==1 | I131_treatment==1 | thyroid_surgery==1 |
    on_antithyroid_medication==1 | pregnant==1 | goitre==1 | lithium==1 |
    TSH_measured==0 | TT4_measured==0 | T3_measured==0 | T4U_measured==0 | FTI_measured==0 
  ) %>% select(-referral_source, -tumor, -I131_treatment, -thyroid_surgery,
               -on_antithyroid_medication, -pregnant, -goitre, -lithium,
               -TSH_measured, -TT4_measured, -T3_measured, -T4U_measured, -FTI_measured)
  
  X2 <- X %>% filter(
    referral_source!="STMW" & tumor!=1 & I131_treatment!=1 & thyroid_surgery!=1 &
    on_antithyroid_medication!=1 & pregnant!=1 & goitre!=1 & lithium!=1 &
    TSH_measured!=0 & TT4_measured!=0 & T3_measured!=0 & T4U_measured!=0 & FTI_measured!=0 
  ) %>% select(-referral_source, -tumor, -I131_treatment, -thyroid_surgery,
               -on_antithyroid_medication, -pregnant, -goitre, -lithium,
               -TSH_measured, -TT4_measured, -T3_measured, -T4U_measured, -FTI_measured)
  
  list(X1, X2)
}

```

It works (rows are the same, columns are not the same)

```{r}
temp <- pp(dataset)
dim(temp[[2]])+dim(temp[[1]])==dim(dataset)
```

### experiment1()

This function uses `pp()` on both `train` and `test` to filter not needed rows. 

`pp()` acts the same for both of the datasets and doesn't use additional information from `test`. 

After that, `auc` and `auprc` are caluclated.

```{r}
experiment1 <- function(trainX, testX) {
  
  ## preprocess
  pp_trainX <- pp(trainX) ## note: pp_trainX[1] is useless
  pp_testX <- pp(testX) 
  
  ## first stage
  first_testX <- pp_testX[[1]]
  first_prob <- rep(mean(first_testX$Class), dim(first_testX)[1])
  first_y_truth <- first_testX$Class
  
  ## second stage
  second_testX <- pp_testX[[2]]
  second_trainX <- pp_trainX[[2]]
  
  model <- rpart(Class~., data = second_trainX)
  second_prob <- predict(model, second_testX)
  second_y_truth <- second_testX$Class
  
  ## glue
  prob <- c(first_prob, second_prob)
  y_truth <- c(first_y_truth, second_y_truth)
  
  ## AUC and AUPRC
  fg <- prob[y_truth == 1]
  bg <- prob[y_truth == 0]
  
  # ROC Curve
  roc <- roc.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
  auc <- roc$auc
  # PR Curve
  pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
  auprc <- pr$auc.integral
  
  list(auc=auc,auprc=auprc)
}

temp <- dataset
temp$Class <- as.numeric(as.character(temp$Class))
train <- temp[index, ]
test <- temp[-index, ]

set.seed(123)
sampling_index <- sample(1:dim(train)[1], dim(train)[1]/5) # take 20%
ttrain <- train[-sampling_index,]
ttest <- train[sampling_index,]

t(data.frame("cv1"=round(unlist(experiment1(ttrain, ttest)),3)))

t(data.frame("test"=round(unlist(experiment1(train, test)),3)))
```

Unfortunatelly this experiment **didn't work** (no point in doing more cv).

## Experiment 2 - Try to upgrade the base model

I tried:

- subseting different groups of columns
- multiple types of imputations
- hyperparameter tuning

First two didn't help at all. Tunning the parameters made the model worse (on test) because it was overfitted to the training data.

# Final model

## CV1 function

```{r}
cv1 <- function(trainX, testX) {
  
  model <- rpart(Class~., data = trainX)
  prob <- predict(model, testX)
  y_truth <- testX$Class
  
  ## AUC and AUPRC
  fg <- prob[y_truth == 1]
  bg <- prob[y_truth == 0]
  
  # ROC Curve
  roc <- roc.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
  auc <- roc$auc
  # PR Curve
  pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
  auprc <- pr$auc.integral
  
  list("auc"=auc, "auprc"=auprc)
}

temp <- dataset
temp$Class <- as.numeric(as.character(temp$Class))
train <- temp[index, ]
test <- temp[-index, ]

results_train <- data.frame()

set.seed(123)

#:# do 5CV on train
for (i in 1:5) {
  sample_index <- sample(1:dim(train)[1], dim(train)[1]/5) # 20%
  ttrain <- train[-sample_index,]
  ttest <- train[sample_index,]
  ret <- cv1(ttrain, ttest)
  results_train <- rbind(results_train, ret)
}
```

## CV5 train

```{r}
t(data.frame("cv5"=round(unlist(colMeans(results_train)), 3)))
```

## test

```{r}
results_test <- cv1(train, test)

t(data.frame("test"=round(unlist(results_test), 3)))
```

## Tree plot

```{r}
model <- rpart(Class~., data = train, model=TRUE)
rpart.plot::rpart.plot(model)
```

 