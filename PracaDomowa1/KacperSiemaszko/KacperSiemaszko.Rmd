---
title: "Sick dataset analysis"
author: "Kacper Siemaszko"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align='center', message = FALSE, warning = FALSE)
set.seed(1)
library(dplyr)
library(data.table)
library(mlr)
library(caret)
library(corrplot)
library(mltools)
library(MASS)
library(DALEX)
library(PRROC)
library(arules)
library(knitr)
library(gridExtra)
```

# EDA

## Initial preprocessing

The first step of exploratory data analysis on OpenML's *sick* dataset will be examining raw data that we've just loaded. By looking at the **nlevs** column, we decide which column we'll transform to binary, numeric or factor type.

* All variables with to nlevs equal 2 will be set to binary, with exception of **Class** which I'll leave as factor. * * Referral_source will be transformed into 5 new columns using one hot encoding. 
* Variables with high number of levels will be transformed to numerics.
* Columns **sex**, **TSH**, **T3**, **T4U**, **FTI**, **TBG** contain missing values, which will be dealt with later.

```{r data, echo = FALSE, results='asis'}
data <- read.csv(file = "./dataset_38_sick.csv")
training_indices <- read.csv(file ="./indeksy_treningowe.txt", sep = " ")
kable(mlr::summarizeColumns(data)[,c("name","type","disp","nlevs")], caption = "Raw sick dataset" )
```

```{r raw_data_transformation, include = FALSE}
transform.raw.to.numeric <- function(data) {
  data$age <- as.numeric(data$age)
  data$sex<-as.numeric(ifelse(data$sex == "?", 1, data$sex == "F"))
  
  data[,colnames(data)[c(3:16,17,19,21,23,25,27)]] <- as.numeric(data[,colnames(data)[c(3:16,17,19,21,23,25,28)]]=='t')
  
    data[,colnames(data)[c(18,20,22,24,26,28)]] <- lapply(data[,colnames(data)[c(18,20,22,24,26,28)]], function(x) as.numeric(ifelse(x == "?", NA, x)))
    
  one.hot.referral_source <- model.matrix(~0+data$referral_source)[,1:5]
  colnames(one.hot.referral_source) <- c("referral_source_other", "referral_source_STMW","referral_source_SVHC","referral_source_SVHD","referral_source_SVI")
  data[,colnames(one.hot.referral_source)] <- one.hot.referral_source
  data[,!(colnames(data) %in% c("referral_source"))]
}
```

After transforming variable to numeric types I have noticed, that three columns have become entirely redundant:

* Columns **TBG** and **TBG_measured** do not contain any data.
* In training indices, column **hypopituitary** contains only one value.

Because of that, I decided to remove them. From this point, exploration will be performed only on the training dataset.

```{r numeric_data, include = FALSE}
numeric_data <- transform.raw.to.numeric(data)

numeric_data <- numeric_data[,!(colnames(data) %in% c("TBG_measured","TBG","hypopituitary"))]

train_data <- numeric_data[training_indices$x,]
test_data <- numeric_data[-training_indices$x,]

train_data2 <- train_data
train_data2$Class <- as.numeric(train_data2$Class == "sick")
```

## Visual exploration of training dataset

### Variable distribution

To examine variable distribution, we'll use binary plot for binary variables and histograms for numerical variables.

```{r binary_data, include=TRUE, echo=FALSE, results='asis'}
dat_bin <- train_data2[,c('sex', 'on_thyroxine', 'query_on_thyroxine', 'on_antithyroid_medication', 'sick', 'pregnant', 'thyroid_surgery', 'I131_treatment', 'query_hypothyroid', 'query_hyperthyroid', 'lithium', 'goitre', 'tumor', 'psych', 'TSH_measured', 'T3_measured', 'TT4_measured', 'T4U_measured', 'FTI_measured','referral_source_other', 'referral_source_STMW','referral_source_SVHC','referral_source_SVHD','referral_source_SVI', 'Class')]
visdat::vis_binary(dat_bin)
```

```{r numerical_data, include=TRUE, echo=FALSE, results='asis'}
p.age <- ggplot(train_data2) + 
  geom_histogram(aes(age, fill=1,alpha=0.75),bins=20,show.legend=FALSE) + 
  theme_minimal()

p.TSH <- ggplot(train_data2) + 
  geom_histogram(aes(TSH, fill=1,alpha=0.75),bins=20,show.legend=FALSE) + 
  theme_minimal()

p.T3 <- ggplot(train_data2) + 
  geom_histogram(aes(T3, fill=1,alpha=0.75),bins=20,show.legend=FALSE) + 
  theme_minimal()

p.TT4 <- ggplot(train_data2) + 
  geom_histogram(aes(TT4, fill=1,alpha=0.75),bins=20,show.legend=FALSE) + 
  theme_minimal()

p.T4U <- ggplot(train_data2) + 
  geom_histogram(aes(T4U, fill=1,alpha=0.75),bins=20,show.legend=FALSE) + 
  theme_minimal()

p.FTI <- ggplot(train_data2) + 
  geom_histogram(aes(FTI, fill=1,alpha=0.75),bins=20,show.legend=FALSE) + 
  theme_minimal()

grid.arrange(p.age,p.TSH,p.T3,p.TT4,p.T4U,p.FTI, nrow = 2)
```

As we can see, binary variables are extremely unbalanced, including our target variable - **Class**. On the other hand, numerical variables like **TT4** or **FTI** have unexpected, bimodal distributions. It should be a clue to look at their behaviour in the next steps.

## Correlation between variables

For the sake of correlation visualization, I will impute missing data with mode/mean of the respective variable. During experiment phase, I'll be testing other imputation techniques

```{r corr, echo=FALSE}
means <- lapply(train_data[,c("TSH","T3","TT4","T4U","FTI")], mean, na.rm = TRUE)

impute.with.mean <- function(data) {
  for (colname in c("TSH","T3","TT4","T4U","FTI")) {
    data[is.na(data[,colname]),colname] <- means[colname]
  }
  data
}

correlation <- cor(impute.with.mean(train_data2))
corrplot(correlation, method="circle", tl.cex = 0.6, tl.col = 'black')
```

By looking at the **Class** correlation row, we find out that **T3** variable has the highest correlation coefficient of all variables. Also, other highly correlated variables, are **_measured** flags, which allows as to suspect, that they may be redundant.

# Experiment
Main idea behind the experiment is preparing a benchmark black box model, to set ourselves **AUC** and **AUPRC** goal. My black box model of choice is **ranger**. Besides a black box, I'll be testing two interpretable machine learning models - **glmnet** and **rpart**.

```{r models, echo=TRUE}
ranger <- makeLearner("classif.ranger", id="ranger", predict.type = "prob", num.threads = 4)
glmnet <- makeLearner("classif.glmnet", id="glmnet", predict.type = "prob")
rpart <- makeLearner("classif.rpart", id="rpart", predict.type = "prob")
```

I'm using **mlr** package as my machine learning toolbox, and one of its pros is possibility to create custom measures. It will allow me to use **auprc** in **mlr::benchmark** and quickly compare many learners trained on different tasks.

```{r auprc, echo=TRUE}
auprcWrapper <- function(task, model, pred, feats, extra.args){
  probs <- getPredictionProbabilities(pred)
  fg <- probs[pred$data$truth == "sick"]
  bg <- probs[pred$data$truth == "negative"]
  pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
  pr$auc.integral
}

measure.auprc <- makeMeasure(id="auprc",minimize = FALSE, properties = c("classif","req.pred","req.truth","req.prob"), fun = auprcWrapper)

```


First, I'll be testing importance of only numeric versus all variables. It will give me a direction, where should I focus with the feature engineering. **Ranger** and **glmnet** do not handle missing values, so again I will be using mean imputation.

```{r numeric_vs_binary, echo=TRUE}
measures.colnames <- c("age","TSH","T3","TT4","T4U","FTI")

no.transforms.train <- makeClassifTask("no transformations",
                                       impute.with.mean(train_data),
                                       target = "Class",
                                       positive = "sick")

only.numeric.train <- makeClassifTask("only numeric",
                                      train_data[,colnames(train_data)=="Class" |
                                                   (colnames(train_data) %in% measures.colnames)] %>% impute.with.mean,
                                      target = "Class",
                                      positive = "sick")
```

```{r crossval1, echo=TRUE}
rdesc <- makeResampleDesc("CV", iters=5)
benchmark.results <- benchmark(learners = list(ranger, rpart, glmnet), 
          tasks = list(no.transforms.train, 
                       only.numeric.train),
          resamplings = rdesc,
          measures= list(measure.auprc,auc))
benchmark.results
plotBMRSummary(benchmark.results) + xlim(0,1)
```

## Numerical variables

As we can see, the result for numerical variables are not that worse from the models with all variables. It's even more signifacnt, if we take into account that the competition is between 6 variable and 30 variable models. 

Let's explore the behaviour of these numeric only models. For this I'll be using feature importance and partial dependency plots from **DALEX** package. 

```{r explaining, echo=FALSE}
dat <- train_data[,colnames(train_data)=="Class" | (colnames(train_data) %in% measures.colnames)] %>% impute.with.mean

explainer.ranger <- explain(mlr::train(ranger, only.numeric.train),
                     data = dat, 
                     y = as.numeric(train_data$Class == "sick"),
                     label = "Ranger",
                     type = "classification",
                     predict_function = function(model, data) as.numeric(predict(model, makeClassifTask("x", data, target = "Class", positive = "sick"))$data$prob.sick))

explainer.glmnet <- explain(mlr::train(glmnet, only.numeric.train),
                            data = dat, 
                            y = as.numeric(train_data$Class == "sick"),
                            label = "Glmnet",
                            type = "classification",
                            predict_function = function(model, data) as.numeric(predict(model, makeClassifTask("x", data, target = "Class", positive = "sick"))$data$prob.sick))

explainer.rpart <- explain(mlr::train(rpart, only.numeric.train),
                            data = dat, 
                            y = as.numeric(train_data$Class == "sick"),
                            label = "Rpart",
                            type = "classification",
                            predict_function = function(model, data) as.numeric(predict(model, makeClassifTask("x", data, target = "Class", positive = "sick"))$data$prob.sick))

grid.arrange(
  plot(variable_importance(explainer.ranger)),
  plot(variable_importance(explainer.glmnet)),
  plot(variable_importance(explainer.rpart)),
  ncol=3
)
plot(model_profile(explainer.ranger))
plot(model_profile(explainer.glmnet))
plot(model_profile(explainer.rpart))
```

As we can see from the plots **T3** is the deciding parameter for all the models. The model complexity is clearly visible on the partial dependency plots. *Ranger*'s plots have the most variance, **Glmnet**'s are the simplest and **Rpart** is somewhere in between. On **Rpart**'s plots, besides **T3** steep influence, we can clearly see **TT4** input into prediction, which isn't represented at all by glmnet.

## Choosing variables

We can also use black box, to find important variables, to make models less complicated. For this, we'll also use **DALEX** and variable importance.

```{r choosing_variables}
dat <- train_data %>% impute.with.mean

explainer.ranger <- explain(mlr::train(ranger, no.transforms.train),
                     data = dat, 
                     y = as.numeric(train_data$Class == "sick"),
                     label = "Ranger",
                     type = "classification",
                     predict_function = function(model, data) as.numeric(predict(model, makeClassifTask("x", data, target = "Class", positive = "sick"))$data$prob.sick))
vi <- variable_importance(explainer.ranger)
plot(vi)
chosen_variables <- as.vector(vi$variable[21:32])
```

To simplify the mode we choose 12 most important variables

```{r update_data}
train_data <- train_data[,c("Class",chosen_variables)]
test_data <- test_data[,c("Class",chosen_variables)]
```

## Imputation

My next step will be testing different imputation techniques

* Imputation with zeros and dropping **_measured** flags
* Imputation with mean and dropping **_measured** flags
* Imputation with mean of negative cases and dropping **_measured** flags
* Numerical variable discretization

```{r imputation_techniques, echo=FALSE}
impute.with.zero <- function(data) {
  
  for (colname in c("TSH","T3","TT4","T4U","FTI")) {
    data[,colname] <- replace_na(data[,colname], 0)
  }
  data
}

means <- lapply(train_data[,c("TSH","T3","TT4","T4U","FTI")], mean, na.rm = TRUE)

impute.with.mean <- function(data) {
  for (colname in c("TSH","T3","TT4","T4U","FTI")) {
    data[is.na(data[,colname]),colname] <- means[colname]
  }
  data
}

means.of.negatives <- lapply(train_data[train_data$Class=="negative",c("TSH","T3","TT4","T4U","FTI")], mean, na.rm = TRUE)

impute.with.mean.of.negatives <- function(data) {
  for (colname in c("TSH","T3","TT4","T4U","FTI")) {
    data[is.na(data[,colname]),colname] <- means.of.negatives[colname]
  }
  data
}

discretizedMeasures <- lapply(train_data[,c("age","TSH","T3","TT4","T4U","FTI")], discretize, breaks=5, infinity=TRUE, onlycuts=TRUE)

numeric.to.discrete <- function(data) {
  for (colname in c("age","TSH","T3","TT4","T4U","FTI")) {
    discrete <- discretize(data[,colname], method = "fixed", breaks = discretizedMeasures[colname][[1]])
    discrete_dummy <- as.data.frame(replace_na(one_hot(as.data.table(discrete)), 0))
    colnames(discrete_dummy) <- stringi::stri_replace_all(str=colnames(discrete_dummy),fixed='discrete',replacement = colname)
    colnames(discrete_dummy) <- stringi::stri_replace_all(str=colnames(discrete_dummy),fixed='_[',replacement = ".from.")
    colnames(discrete_dummy) <- stringi::stri_replace_all(str=colnames(discrete_dummy),fixed=',' ,replacement = ".to.")
    colnames(discrete_dummy) <- stringi::stri_replace_all(str=colnames(discrete_dummy),fixed=' ' ,replacement = "")
    colnames(discrete_dummy) <- stringi::stri_replace_all(str=colnames(discrete_dummy),fixed=')' ,replacement = "")
    colnames(discrete_dummy) <- stringi::stri_replace_all(str=colnames(discrete_dummy),fixed=']' ,replacement = "")
    colnames(discrete_dummy) <- stringi::stri_replace_all(str=colnames(discrete_dummy),fixed='-' ,replacement = "minus.")

    data[,colnames(discrete_dummy)] <- discrete_dummy
    data <- data[,colnames(data) != colname]
  }
  data
}
```

```{r imputation_tasks}
with.zero <- makeClassifTask("with zero", impute.with.zero(train_data), target = "Class", positive = "sick")
with.mean <- makeClassifTask("with mean", impute.with.mean(train_data), target = "Class", positive = "sick")
with.mean.of.negatives <- makeClassifTask("with mean of negatives",impute.with.mean.of.negatives(train_data), target = "Class", positive = "sick")

num.to.discrete <- makeClassifTask("numeric to discrete", numeric.to.discrete(train_data), target = "Class", positive = "sick")

rdesc <- makeResampleDesc("CV", iters=5)
benchmark.results <- benchmark(learners = list(ranger, rpart, glmnet), 
          tasks = list(with.zero, 
                       with.mean,
                       with.mean.of.negatives,
                       num.to.discrete),
          resamplings = rdesc,
          measures= list(measure.auprc,auc))
benchmark.results
plotBMRSummary(benchmark.results) + xlim(0,1)
```

## Conclusions 

Changing numeric variables to discrete form didn't have a positive effect on crossvalidation **AUPRC**. From this we can conclude, that there is little non-linearity connected with single variable transformation. More likely it's variables interacting with each other. At this point, adding variables that would symbolize these interactions, would only complicate the model and make it less understandable. Experiments lead me to choosing **rpart** as my interpretable model, with missing numerical values inputed with mean of negative cases.

```{r final_tree}
final.model <- mlr::train(rpart, with.mean.of.negatives)
rpart.plot::rpart.plot(getLearnerModel(final.model))
```

Final AUPRC on test dataset

```{r final.auprc}
test.task <- makeClassifTask("with mean of negatives",impute.with.mean.of.negatives(test_data), target = "Class", positive = "sick")

auprcWrapper(-1,-1,predict(final.model, test.task),-1,-1)
```

Final AUC on test dataset
```{r final.auc}
test.task <- makeClassifTask("with mean of negatives",impute.with.mean.of.negatives(test_data), target = "Class", positive = "sick")
pred <- predict(final.model, test.task)
measureAUC(pred$data$prob.sick,pred$data$truth,"negative","sick")
```
